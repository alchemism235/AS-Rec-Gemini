{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2115bf36-28c3-42f4-8ac0-769917195353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sentiment data from 'review_business_5up_5aspect_3sentiment_vectorized_clean.json'...\n",
      "Sentiment data loaded. Total records: 447796\n",
      "Data processed. Total records: 447796\n",
      "전체 데이터 수: 447796\n",
      "학습 데이터 수: 313456 (70.00%)\n",
      "검증 데이터 수: 44780 (10.00%)\n",
      "테스트 데이터 수: 89560 (20.00%)\n",
      "\n",
      "==================================================\n",
      "Best Parameters: {'aspect_mlp_hidden_dims': [64, 32], 'batch_size': 128, 'embedding_dim': 64, 'final_mlp_hidden_dims': [32, 16], 'learning_rate': 0.001, 'user_biz_mlp_hidden_dims': [128, 64]}\n",
      "==================================================\n",
      "\n",
      "--- Training Final Model with Best Parameters ---\n",
      "Epoch 1/50\n",
      "\u001b[1m2437/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.1951 - mae: 0.7401 - rmse: 1.0054\n",
      "Epoch 1: val_rmse improved from inf to 0.67721, saving model to final_best_as_rec_model_sentiment_only.keras\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - loss: 1.1920 - mae: 0.7392 - rmse: 1.0041 - val_loss: 0.4586 - val_mae: 0.5305 - val_rmse: 0.6772\n",
      "Epoch 2/50\n",
      "\u001b[1m2446/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4040 - mae: 0.4931 - rmse: 0.6356\n",
      "Epoch 2: val_rmse improved from 0.67721 to 0.67320, saving model to final_best_as_rec_model_sentiment_only.keras\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - loss: 0.4040 - mae: 0.4931 - rmse: 0.6356 - val_loss: 0.4532 - val_mae: 0.5192 - val_rmse: 0.6732\n",
      "Epoch 3/50\n",
      "\u001b[1m2447/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3076 - mae: 0.4197 - rmse: 0.5546\n",
      "Epoch 3: val_rmse did not improve from 0.67320\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - loss: 0.3077 - mae: 0.4197 - rmse: 0.5546 - val_loss: 0.4833 - val_mae: 0.5260 - val_rmse: 0.6952\n",
      "Epoch 4/50\n",
      "\u001b[1m2446/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2033 - mae: 0.3300 - rmse: 0.4507\n",
      "Epoch 4: val_rmse did not improve from 0.67320\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - loss: 0.2033 - mae: 0.3301 - rmse: 0.4508 - val_loss: 0.5241 - val_mae: 0.5434 - val_rmse: 0.7240\n",
      "Epoch 5/50\n",
      "\u001b[1m2447/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1486 - mae: 0.2716 - rmse: 0.3854\n",
      "Epoch 5: val_rmse did not improve from 0.67320\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - loss: 0.1487 - mae: 0.2717 - rmse: 0.3854 - val_loss: 0.5519 - val_mae: 0.5468 - val_rmse: 0.7429\n",
      "Epoch 6/50\n",
      "\u001b[1m2439/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1194 - mae: 0.2349 - rmse: 0.3454\n",
      "Epoch 6: val_rmse did not improve from 0.67320\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - loss: 0.1195 - mae: 0.2350 - rmse: 0.3455 - val_loss: 0.5490 - val_mae: 0.5446 - val_rmse: 0.7409\n",
      "Epoch 7/50\n",
      "\u001b[1m2446/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1002 - mae: 0.2085 - rmse: 0.3164\n",
      "Epoch 7: val_rmse did not improve from 0.67320\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - loss: 0.1002 - mae: 0.2085 - rmse: 0.3164 - val_loss: 0.5807 - val_mae: 0.5549 - val_rmse: 0.7621\n",
      "Epoch 8/50\n",
      "\u001b[1m2445/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0871 - mae: 0.1884 - rmse: 0.2950\n",
      "Epoch 8: val_rmse did not improve from 0.67320\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - loss: 0.0871 - mae: 0.1884 - rmse: 0.2951 - val_loss: 0.5763 - val_mae: 0.5569 - val_rmse: 0.7592\n",
      "Epoch 9/50\n",
      "\u001b[1m2437/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0777 - mae: 0.1738 - rmse: 0.2786\n",
      "Epoch 9: val_rmse did not improve from 0.67320\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - loss: 0.0777 - mae: 0.1739 - rmse: 0.2786 - val_loss: 0.5956 - val_mae: 0.5623 - val_rmse: 0.7718\n",
      "Epoch 10/50\n",
      "\u001b[1m2443/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0694 - mae: 0.1608 - rmse: 0.2634\n",
      "Epoch 10: val_rmse did not improve from 0.67320\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - loss: 0.0695 - mae: 0.1608 - rmse: 0.2634 - val_loss: 0.5757 - val_mae: 0.5459 - val_rmse: 0.7588\n",
      "Epoch 11/50\n",
      "\u001b[1m2442/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0635 - mae: 0.1496 - rmse: 0.2520\n",
      "Epoch 11: val_rmse did not improve from 0.67320\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - loss: 0.0636 - mae: 0.1496 - rmse: 0.2521 - val_loss: 0.6000 - val_mae: 0.5579 - val_rmse: 0.7746\n",
      "Epoch 12/50\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0585 - mae: 0.1416 - rmse: 0.2419\n",
      "Epoch 12: val_rmse did not improve from 0.67320\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - loss: 0.0585 - mae: 0.1416 - rmse: 0.2419 - val_loss: 0.5928 - val_mae: 0.5475 - val_rmse: 0.7699\n",
      "\n",
      "--- Evaluating Final Model on Test Set ---\n",
      "Loaded best model weights from final_best_as_rec_model_sentiment_only.keras\n",
      "\u001b[1m2799/2799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 568us/step\n",
      "\n",
      "--- Performance Evaluation (Final Model with Best Parameters) ---\n",
      "Selected Hyperparameters: {'aspect_mlp_hidden_dims': [64, 32], 'batch_size': 128, 'embedding_dim': 64, 'final_mlp_hidden_dims': [32, 16], 'learning_rate': 0.001, 'user_biz_mlp_hidden_dims': [128, 64]}\n",
      "Mean Squared Error (MSE): 0.4471\n",
      "Root Mean Squared Error (RMSE): 0.6686\n",
      "Mean Absolute Error (MAE): 0.5169\n",
      "Mean Absolute Percentage Error (MAPE): 17.90%\n"
     ]
    }
   ],
   "source": [
    "#기존 \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "\n",
    "\n",
    "# MAPE 오류 방지\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    non_zero_true = y_true != 0\n",
    "    if np.sum(non_zero_true) == 0:\n",
    "        return 0.0 # 모든 y_true가 0인 경우 MAPE는 0으로 처리\n",
    "    return np.mean(np.abs((y_true[non_zero_true] - y_pred[non_zero_true]) / y_true[non_zero_true])) * 100\n",
    "\n",
    "\n",
    "# ABSA 15차원\n",
    "print(\"Loading sentiment data from 'review_business_5up_5aspect_3sentiment_vectorized_clean.json'...\")\n",
    "df = pd.read_json('review_business_5up_5aspect_3sentiment_vectorized_clean.json', lines=True)\n",
    "print(f\"Sentiment data loaded. Total records: {len(df)}\")\n",
    "\n",
    "df_processed = df[['user_id', 'business_id', 'stars', 'sentiment_vector']].copy()\n",
    "print(f\"Data processed. Total records: {len(df_processed)}\")\n",
    "\n",
    "user_encoder = LabelEncoder()\n",
    "business_encoder = LabelEncoder()\n",
    "\n",
    "df_processed.loc[:, 'user_encoded'] = user_encoder.fit_transform(df_processed['user_id'])\n",
    "df_processed.loc[:, 'business_encoded'] = business_encoder.fit_transform(df_processed['business_id'])\n",
    "\n",
    "num_users = len(user_encoder.classes_)\n",
    "num_businesses = len(business_encoder.classes_)\n",
    "\n",
    "# 데이터 7:1:2로 분할 \n",
    "train_val_df, test_df = train_test_split(df_processed, test_size=0.2, random_state=42)\n",
    "val_size_ratio = 1 / 8 # 10% of total data (1/8 of 80%)\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=val_size_ratio, random_state=42)\n",
    "\n",
    "print(f\"전체 데이터 수: {len(df_processed)}\")\n",
    "print(f\"학습 데이터 수: {len(train_df)} ({len(train_df)/len(df_processed)*100:.2f}%)\")\n",
    "print(f\"검증 데이터 수: {len(val_df)} ({len(val_df)/len(df_processed)*100:.2f}%)\")\n",
    "print(f\"테스트 데이터 수: {len(test_df)} ({len(test_df)/len(df_processed)*100:.2f}%)\")\n",
    "\n",
    "\n",
    "# 임베딩 차원 설정\n",
    "sentiment_vector_dim = len(df_processed['sentiment_vector'].iloc[0]) if not df_processed.empty else 15\n",
    "\n",
    "\n",
    "train_sentiment_vectors = np.array(train_df['sentiment_vector'].tolist())\n",
    "val_sentiment_vectors = np.array(val_df['sentiment_vector'].tolist())\n",
    "test_sentiment_vectors = np.array(test_df['sentiment_vector'].tolist())\n",
    "\n",
    "\n",
    "def build_asrec_model(num_users, num_businesses, embedding_dim,\n",
    "                       user_biz_mlp_dims, aspect_mlp_dims, final_mlp_dims,\n",
    "                       sentiment_vector_dim):\n",
    "\n",
    "    # 유저-비즈니스 관계 모듈\n",
    "    user_input = keras.Input(shape=(1,), name='user_id')\n",
    "    business_input = keras.Input(shape=(1,), name='business_id')\n",
    "\n",
    "    user_embedding = layers.Embedding(num_users, embedding_dim, name='user_embedding')(user_input)\n",
    "    user_vec = layers.Flatten()(user_embedding)\n",
    "\n",
    "    business_embedding = layers.Embedding(num_businesses, embedding_dim, name='business_embedding')(business_input)\n",
    "    business_vec = layers.Flatten()(business_embedding)\n",
    "\n",
    "    combined_vec = layers.concatenate([user_vec, business_vec], axis=1)\n",
    "\n",
    "    interaction_features = combined_vec\n",
    "    for dim in user_biz_mlp_dims:\n",
    "        interaction_features = layers.Dense(dim, activation='relu')(interaction_features)\n",
    "\n",
    "    # ABSA 모듈\n",
    "    sentiment_input = keras.Input(shape=(sentiment_vector_dim,), name='sentiment_vector')\n",
    "\n",
    "    aspect_features = sentiment_input\n",
    "    for dim in aspect_mlp_dims:\n",
    "        aspect_features = layers.Dense(dim, activation='relu')(aspect_features)\n",
    "\n",
    "    # 최종 예측 모듈\n",
    "    final_combined_features = layers.concatenate([interaction_features, aspect_features], axis=1)\n",
    "\n",
    "    predicted_rating = final_combined_features\n",
    "    for dim in final_mlp_dims:\n",
    "        predicted_rating = layers.Dense(dim, activation='relu')(predicted_rating)\n",
    "    predicted_rating = layers.Dense(1, activation='linear', name='output_rating')(predicted_rating)\n",
    "\n",
    "    \n",
    "    model = models.Model(inputs=[user_input, business_input, sentiment_input],\n",
    "                         outputs=predicted_rating)\n",
    "    return model\n",
    "\n",
    "# 파라미터\n",
    "best_params = {\n",
    "    'aspect_mlp_hidden_dims': [64, 32],\n",
    "    'batch_size': 128,\n",
    "    'embedding_dim': 64,\n",
    "    'final_mlp_hidden_dims': [32, 16],\n",
    "    'learning_rate': 0.001,\n",
    "    'user_biz_mlp_hidden_dims': [128, 64]\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "\n",
    "final_embedding_dim = best_params['embedding_dim']\n",
    "final_learning_rate = best_params['learning_rate']\n",
    "final_batch_size = best_params['batch_size']\n",
    "final_user_biz_mlp_dims = best_params['user_biz_mlp_hidden_dims']\n",
    "final_aspect_mlp_dims = best_params['aspect_mlp_hidden_dims']\n",
    "final_final_mlp_dims = best_params['final_mlp_hidden_dims']\n",
    "\n",
    "# 최종 모델\n",
    "final_model = build_asrec_model(num_users, num_businesses, final_embedding_dim,\n",
    "                                 final_user_biz_mlp_dims, final_aspect_mlp_dims, final_final_mlp_dims,\n",
    "                                 sentiment_vector_dim)\n",
    "\n",
    "final_model.compile(optimizer=keras.optimizers.Adam(learning_rate=final_learning_rate),\n",
    "                    loss='mse',\n",
    "                    metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse'), 'mae'])\n",
    "\n",
    "final_model_path = 'final_best_as_rec_model_sentiment_only.keras'\n",
    "\n",
    "early_stopping_callback = callbacks.EarlyStopping(\n",
    "    monitor='val_rmse',\n",
    "    patience=10,\n",
    "    min_delta=0.0005,\n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "model_checkpoint_callback = callbacks.ModelCheckpoint(\n",
    "    filepath=final_model_path,\n",
    "    monitor='val_rmse',\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n--- Training Final Model with Best Parameters ---\")\n",
    "history = final_model.fit(\n",
    "    {'user_id': train_df['user_encoded'],\n",
    "     'business_id': train_df['business_encoded'],\n",
    "     'sentiment_vector': train_sentiment_vectors},\n",
    "    train_df['stars'],\n",
    "    batch_size=final_batch_size,\n",
    "    epochs=50, \n",
    "    validation_data=(\n",
    "        {'user_id': val_df['user_encoded'],\n",
    "         'business_id': val_df['business_encoded'],\n",
    "         'sentiment_vector': val_sentiment_vectors},\n",
    "        val_df['stars']\n",
    "    ),\n",
    "    callbacks=[early_stopping_callback, model_checkpoint_callback],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n--- Evaluating Final Model on Test Set ---\")\n",
    "if os.path.exists(final_model_path):\n",
    "    final_model = keras.models.load_model(final_model_path)\n",
    "    print(f\"Loaded best model weights from {final_model_path}\")\n",
    "else:\n",
    "    print(f\"Could not find optimal final model weights at '{final_model_path}'. Testing with current model state.\")\n",
    "\n",
    "test_predictions = final_model.predict(\n",
    "    {'user_id': test_df['user_encoded'],\n",
    "     'business_id': test_df['business_encoded'],\n",
    "     'sentiment_vector': test_sentiment_vectors}\n",
    ").flatten()\n",
    "\n",
    "true_ratings = test_df['stars'].values\n",
    "\n",
    "mse = mean_squared_error(true_ratings, test_predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(true_ratings, test_predictions)\n",
    "mape = mean_absolute_percentage_error(true_ratings, test_predictions)\n",
    "\n",
    "print(f\"\\n--- Performance Evaluation (Final Model with Best Parameters) ---\")\n",
    "print(f\"Selected Hyperparameters: {best_params}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")\n",
    "\n",
    "\n",
    "if os.path.exists('temp_models'):\n",
    "    import shutil\n",
    "    shutil.rmtree('temp_models')\n",
    "    print(\"\\nCleaned up 'temp_models' directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ab03a92-8cfd-4798-a4ac-ccecfc4531c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sentiment data from 'review_business_5up_5aspect_3sentiment_vectorized_clean.json'...\n",
      "Sentiment data loaded. Total records: 447796\n",
      "Loading reduced embedding data from 'review_business_5up_with_reduced_embedding.jsonl'...\n",
      "Reduced embedding data loaded. Total records: 447796\n",
      "Data merged successfully. Total processed records: 447796\n",
      "전체 데이터 수: 447796\n",
      "학습 데이터 수: 313456 (70.00%)\n",
      "검증 데이터 수: 44780 (10.00%)\n",
      "테스트 데이터 수: 89560 (20.00%)\n",
      "\n",
      "==================================================\n",
      "Best Parameters: {'aspect_mlp_hidden_dims': [64, 32], 'batch_size': 128, 'embedding_dim': 64, 'final_mlp_hidden_dims': [32, 16], 'learning_rate': 0.001, 'user_biz_mlp_hidden_dims': [128, 64]}\n",
      "==================================================\n",
      "\n",
      "--- Training Final Model with Best Parameters ---\n",
      "Epoch 1/50\n",
      "\u001b[1m2445/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.5642 - mae: 0.8160 - rmse: 1.1362\n",
      "Epoch 1: val_rmse improved from inf to 0.67386, saving model to final_best_as_rec_model_with_pre_reduced_embedding.keras\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - loss: 1.5624 - mae: 0.8156 - rmse: 1.1356 - val_loss: 0.4541 - val_mae: 0.5249 - val_rmse: 0.6739\n",
      "Epoch 2/50\n",
      "\u001b[1m2441/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4032 - mae: 0.4906 - rmse: 0.6350\n",
      "Epoch 2: val_rmse improved from 0.67386 to 0.66766, saving model to final_best_as_rec_model_with_pre_reduced_embedding.keras\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - loss: 0.4032 - mae: 0.4906 - rmse: 0.6350 - val_loss: 0.4458 - val_mae: 0.5099 - val_rmse: 0.6677\n",
      "Epoch 3/50\n",
      "\u001b[1m2447/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3232 - mae: 0.4315 - rmse: 0.5685\n",
      "Epoch 3: val_rmse did not improve from 0.66766\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - loss: 0.3232 - mae: 0.4316 - rmse: 0.5685 - val_loss: 0.4561 - val_mae: 0.5181 - val_rmse: 0.6753\n",
      "Epoch 4/50\n",
      "\u001b[1m2444/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2234 - mae: 0.3499 - rmse: 0.4725\n",
      "Epoch 4: val_rmse did not improve from 0.66766\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - loss: 0.2234 - mae: 0.3499 - rmse: 0.4726 - val_loss: 0.4946 - val_mae: 0.5307 - val_rmse: 0.7033\n",
      "Epoch 5/50\n",
      "\u001b[1m2448/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1590 - mae: 0.2858 - rmse: 0.3986\n",
      "Epoch 5: val_rmse did not improve from 0.66766\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - loss: 0.1590 - mae: 0.2859 - rmse: 0.3986 - val_loss: 0.5225 - val_mae: 0.5389 - val_rmse: 0.7228\n",
      "Epoch 6/50\n",
      "\u001b[1m2444/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1249 - mae: 0.2481 - rmse: 0.3533\n",
      "Epoch 6: val_rmse did not improve from 0.66766\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - loss: 0.1250 - mae: 0.2481 - rmse: 0.3533 - val_loss: 0.5416 - val_mae: 0.5402 - val_rmse: 0.7359\n",
      "Epoch 7/50\n",
      "\u001b[1m2446/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1039 - mae: 0.2207 - rmse: 0.3222\n",
      "Epoch 7: val_rmse did not improve from 0.66766\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - loss: 0.1039 - mae: 0.2207 - rmse: 0.3222 - val_loss: 0.5506 - val_mae: 0.5417 - val_rmse: 0.7420\n",
      "Epoch 8/50\n",
      "\u001b[1m2444/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0902 - mae: 0.1993 - rmse: 0.3002\n",
      "Epoch 8: val_rmse did not improve from 0.66766\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - loss: 0.0902 - mae: 0.1993 - rmse: 0.3002 - val_loss: 0.5616 - val_mae: 0.5497 - val_rmse: 0.7494\n",
      "Epoch 9/50\n",
      "\u001b[1m2441/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0765 - mae: 0.1761 - rmse: 0.2764\n",
      "Epoch 9: val_rmse did not improve from 0.66766\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - loss: 0.0765 - mae: 0.1761 - rmse: 0.2765 - val_loss: 0.5626 - val_mae: 0.5520 - val_rmse: 0.7500\n",
      "Epoch 10/50\n",
      "\u001b[1m2437/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0687 - mae: 0.1603 - rmse: 0.2620\n",
      "Epoch 10: val_rmse did not improve from 0.66766\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - loss: 0.0687 - mae: 0.1603 - rmse: 0.2621 - val_loss: 0.5689 - val_mae: 0.5385 - val_rmse: 0.7543\n",
      "Epoch 11/50\n",
      "\u001b[1m2448/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0624 - mae: 0.1480 - rmse: 0.2497\n",
      "Epoch 11: val_rmse did not improve from 0.66766\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - loss: 0.0624 - mae: 0.1480 - rmse: 0.2497 - val_loss: 0.5720 - val_mae: 0.5350 - val_rmse: 0.7563\n",
      "Epoch 12/50\n",
      "\u001b[1m2444/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0565 - mae: 0.1362 - rmse: 0.2375\n",
      "Epoch 12: val_rmse did not improve from 0.66766\n",
      "\u001b[1m2449/2449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - loss: 0.0565 - mae: 0.1362 - rmse: 0.2376 - val_loss: 0.5689 - val_mae: 0.5385 - val_rmse: 0.7542\n",
      "\n",
      "--- Evaluating Final Model on Test Set ---\n",
      "Loaded best model weights from final_best_as_rec_model_with_pre_reduced_embedding.keras\n",
      "\u001b[1m2799/2799\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 633us/step\n",
      "\n",
      "--- Performance Evaluation (Final Model with Best Parameters) ---\n",
      "Selected Hyperparameters: {'aspect_mlp_hidden_dims': [64, 32], 'batch_size': 128, 'embedding_dim': 64, 'final_mlp_hidden_dims': [32, 16], 'learning_rate': 0.001, 'user_biz_mlp_hidden_dims': [128, 64]}\n",
      "Mean Squared Error (MSE): 0.4405\n",
      "Root Mean Squared Error (RMSE): 0.6637\n",
      "Mean Absolute Error (MAE): 0.5079\n",
      "Mean Absolute Percentage Error (MAPE): 17.81%\n"
     ]
    }
   ],
   "source": [
    "# 제미니\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "\n",
    "\n",
    "# MAPE 오류 방지\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    non_zero_true = y_true != 0\n",
    "    if np.sum(non_zero_true) == 0:\n",
    "        return 0.0 # 모든 y_true가 0인 경우 MAPE는 0으로 처리\n",
    "    return np.mean(np.abs((y_true[non_zero_true] - y_pred[non_zero_true]) / y_true[non_zero_true])) * 100\n",
    "\n",
    "\n",
    "\n",
    "# ABSA 15차원\n",
    "print(\"Loading sentiment data from 'review_business_5up_5aspect_3sentiment_vectorized_clean.json'...\")\n",
    "df_sentiment = pd.read_json('review_business_5up_5aspect_3sentiment_vectorized_clean.json', lines=True)\n",
    "print(f\"Sentiment data loaded. Total records: {len(df_sentiment)}\")\n",
    "\n",
    "# 제미니 임베딩 32차원\n",
    "print(\"Loading reduced embedding data from 'review_business_5up_with_reduced_embedding.jsonl'...\")\n",
    "df_reduced_emb = pd.read_json('review_business_5up_with_reduced_embedding.jsonl', lines=True)\n",
    "print(f\"Reduced embedding data loaded. Total records: {len(df_reduced_emb)}\")\n",
    "\n",
    "# 데이터 병합\n",
    "df_processed = pd.merge(\n",
    "    df_sentiment[['review_id', 'user_id', 'business_id', 'stars', 'sentiment_vector']],\n",
    "    df_reduced_emb[['review_id', 'reduced_embedding']],\n",
    "    on='review_id',\n",
    "    how='inner'\n",
    ")\n",
    "print(f\"Data merged successfully. Total processed records: {len(df_processed)}\")\n",
    "\n",
    "\n",
    "user_encoder = LabelEncoder()\n",
    "business_encoder = LabelEncoder()\n",
    "\n",
    "df_processed.loc[:, 'user_encoded'] = user_encoder.fit_transform(df_processed['user_id'])\n",
    "df_processed.loc[:, 'business_encoded'] = business_encoder.fit_transform(df_processed['business_id'])\n",
    "\n",
    "num_users = len(user_encoder.classes_)\n",
    "num_businesses = len(business_encoder.classes_)\n",
    "\n",
    "# 데이터 7:1:2로 분할 \n",
    "train_val_df, test_df = train_test_split(df_processed, test_size=0.2, random_state=42)\n",
    "val_size_ratio = 1 / 8 # 데이터의 10%\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=val_size_ratio, random_state=42)\n",
    "\n",
    "print(f\"전체 데이터 수: {len(df_processed)}\")\n",
    "print(f\"학습 데이터 수: {len(train_df)} ({len(train_df)/len(df_processed)*100:.2f}%)\")\n",
    "print(f\"검증 데이터 수: {len(val_df)} ({len(val_df)/len(df_processed)*100:.2f}%)\")\n",
    "print(f\"테스트 데이터 수: {len(test_df)} ({len(test_df)/len(df_processed)*100:.2f}%)\")\n",
    "\n",
    "# 임베딩 차원 설정\n",
    "original_sentiment_dim = len(df_processed['sentiment_vector'].iloc[0]) if not df_processed.empty else 15 # 기존 15차원 감성 벡터\n",
    "loaded_embedding_dim = len(df_processed['reduced_embedding'].iloc[0]) if not df_processed.empty else 32 # 이미 32차원으로 축소된 임베딩\n",
    "\n",
    "train_sentiment_vectors = np.array(train_df['sentiment_vector'].tolist())\n",
    "train_embeddings = np.array(train_df['reduced_embedding'].tolist())\n",
    "val_sentiment_vectors = np.array(val_df['sentiment_vector'].tolist())\n",
    "val_embeddings = np.array(val_df['reduced_embedding'].tolist())\n",
    "test_sentiment_vectors = np.array(test_df['sentiment_vector'].tolist())\n",
    "test_embeddings = np.array(test_df['reduced_embedding'].tolist())\n",
    "\n",
    "\n",
    "def build_asrec_model(num_users, num_businesses, embedding_dim,\n",
    "                       user_biz_mlp_dims, aspect_mlp_dims, final_mlp_dims,\n",
    "                       original_sentiment_dim, loaded_embedding_dim): # loaded_embedding_dim 인자 유지\n",
    "\n",
    "    # 유저-비즈니스 관계 모듈\n",
    "    user_input = keras.Input(shape=(1,), name='user_id')\n",
    "    business_input = keras.Input(shape=(1,), name='business_id')\n",
    "\n",
    "    user_embedding = layers.Embedding(num_users, embedding_dim, name='user_embedding')(user_input)\n",
    "    user_vec = layers.Flatten()(user_embedding)\n",
    "\n",
    "    business_embedding = layers.Embedding(num_businesses, embedding_dim, name='business_embedding')(business_input)\n",
    "    business_vec = layers.Flatten()(business_embedding)\n",
    "\n",
    "    combined_vec = layers.concatenate([user_vec, business_vec], axis=1)\n",
    "\n",
    "    interaction_features = combined_vec\n",
    "    for dim in user_biz_mlp_dims:\n",
    "        interaction_features = layers.Dense(dim, activation='relu')(interaction_features)\n",
    "\n",
    "    # ABSA 모듈\n",
    "    sentiment_input = keras.Input(shape=(original_sentiment_dim,), name='sentiment_vector')\n",
    "    embedding_input = keras.Input(shape=(loaded_embedding_dim,), name='reduced_embedding') # reduced_embedding 입력 다시 추가\n",
    "\n",
    "    combined_aspect_features = layers.concatenate([sentiment_input, embedding_input], axis=1) # 두 임베딩 결합\n",
    "\n",
    "    aspect_features = combined_aspect_features\n",
    "    for dim in aspect_mlp_dims:\n",
    "        aspect_features = layers.Dense(dim, activation='relu')(aspect_features)\n",
    "\n",
    "    # 최종 예측 모듈\n",
    "    final_combined_features = layers.concatenate([interaction_features, aspect_features], axis=1)\n",
    "\n",
    "    predicted_rating = final_combined_features\n",
    "    for dim in final_mlp_dims:\n",
    "        predicted_rating = layers.Dense(dim, activation='relu')(predicted_rating)\n",
    "    predicted_rating = layers.Dense(1, activation='linear', name='output_rating')(predicted_rating)\n",
    "\n",
    "    model = models.Model(inputs=[user_input, business_input, sentiment_input, embedding_input],\n",
    "                         outputs=predicted_rating)\n",
    "    return model\n",
    "\n",
    "# 파라미터\n",
    "best_params = {\n",
    "    'aspect_mlp_hidden_dims': [64, 32],\n",
    "    'batch_size': 128,\n",
    "    'embedding_dim': 64,\n",
    "    'final_mlp_hidden_dims': [32, 16],\n",
    "    'learning_rate': 0.001,\n",
    "    'user_biz_mlp_hidden_dims': [128, 64]\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "final_embedding_dim = best_params['embedding_dim']\n",
    "final_learning_rate = best_params['learning_rate']\n",
    "final_batch_size = best_params['batch_size']\n",
    "final_user_biz_mlp_dims = best_params['user_biz_mlp_hidden_dims']\n",
    "final_aspect_mlp_dims = best_params['aspect_mlp_hidden_dims']\n",
    "final_final_mlp_dims = best_params['final_mlp_hidden_dims']\n",
    "\n",
    "\n",
    "final_model = build_asrec_model(num_users, num_businesses, final_embedding_dim,\n",
    "                                 final_user_biz_mlp_dims, final_aspect_mlp_dims, final_final_mlp_dims,\n",
    "                                 original_sentiment_dim, loaded_embedding_dim)\n",
    "\n",
    "final_model.compile(optimizer=keras.optimizers.Adam(learning_rate=final_learning_rate),\n",
    "                    loss='mse',\n",
    "                    metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse'), 'mae'])\n",
    "\n",
    "final_model_path = 'final_best_as_rec_model_with_pre_reduced_embedding.keras'\n",
    "\n",
    "early_stopping_callback = callbacks.EarlyStopping(\n",
    "    monitor='val_rmse',\n",
    "    patience=10,\n",
    "    min_delta=0.0005,\n",
    "    mode='min',\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "model_checkpoint_callback = callbacks.ModelCheckpoint(\n",
    "    filepath=final_model_path,\n",
    "    monitor='val_rmse',\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n--- Training Final Model with Best Parameters ---\")\n",
    "history = final_model.fit(\n",
    "    {'user_id': train_df['user_encoded'],\n",
    "     'business_id': train_df['business_encoded'],\n",
    "     'sentiment_vector': train_sentiment_vectors,\n",
    "     'reduced_embedding': train_embeddings},\n",
    "    train_df['stars'],\n",
    "    batch_size=final_batch_size,\n",
    "    epochs=50,\n",
    "    validation_data=(\n",
    "        {'user_id': val_df['user_encoded'],\n",
    "         'business_id': val_df['business_encoded'],\n",
    "         'sentiment_vector': val_sentiment_vectors,\n",
    "         'reduced_embedding': val_embeddings},\n",
    "        val_df['stars']\n",
    "    ),\n",
    "    callbacks=[early_stopping_callback, model_checkpoint_callback],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n--- Evaluating Final Model on Test Set ---\")\n",
    "if os.path.exists(final_model_path):\n",
    "    final_model = keras.models.load_model(final_model_path)\n",
    "    print(f\"Loaded best model weights from {final_model_path}\")\n",
    "else:\n",
    "    print(f\"Could not find optimal final model weights at '{final_model_path}'. Testing with current model state.\")\n",
    "\n",
    "test_predictions = final_model.predict(\n",
    "    {'user_id': test_df['user_encoded'],\n",
    "     'business_id': test_df['business_encoded'],\n",
    "     'sentiment_vector': test_sentiment_vectors,\n",
    "     'reduced_embedding': test_embeddings}\n",
    ").flatten()\n",
    "\n",
    "true_ratings = test_df['stars'].values\n",
    "\n",
    "mse = mean_squared_error(true_ratings, test_predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(true_ratings, test_predictions)\n",
    "mape = mean_absolute_percentage_error(true_ratings, test_predictions)\n",
    "\n",
    "print(f\"\\n--- Performance Evaluation (Final Model with Best Parameters) ---\")\n",
    "print(f\"Selected Hyperparameters: {best_params}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")\n",
    "\n",
    "if os.path.exists('temp_models'):\n",
    "    import shutil\n",
    "    shutil.rmtree('temp_models')\n",
    "    print(\"\\nCleaned up 'temp_models' directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e120c5e-5942-4911-a1c0-ca71f22a1b0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
