{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9231b097-62cf-4f29-a961-560e6022f329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Vocabulary size: 51962\n",
      "\n",
      "--- Starting Hyperparameter Search with 10 trials ---\n",
      "Data Split Random State: 42\n",
      "Fixed Data Split: Train=313456, Val=44780, Test=89560\n",
      "\n",
      "--- Trial 1/10 ---\n",
      "Current Parameters: {'embedding_dim': 64, 'text_encoder_hidden_dim': 256, 'learning_rate': 0.002, 'batch_size': 512, 'user_item_mlp_dims': [128, 64], 'final_mlp_dims': [32, 16], 'dropout_rate': 0.3}\n",
      "  Trial 1 Test RMSE: 0.9531\n",
      "  --> New best RMSE found: 0.9531 with params: {'embedding_dim': 64, 'text_encoder_hidden_dim': 256, 'learning_rate': 0.002, 'batch_size': 512, 'user_item_mlp_dims': [128, 64], 'final_mlp_dims': [32, 16], 'dropout_rate': 0.3}\n",
      "\n",
      "--- Trial 2/10 ---\n",
      "Current Parameters: {'embedding_dim': 128, 'text_encoder_hidden_dim': 256, 'learning_rate': 0.001, 'batch_size': 256, 'user_item_mlp_dims': [128, 64], 'final_mlp_dims': [64, 32], 'dropout_rate': 0.1}\n",
      "  Trial 2 Test RMSE: 0.9001\n",
      "  --> New best RMSE found: 0.9001 with params: {'embedding_dim': 128, 'text_encoder_hidden_dim': 256, 'learning_rate': 0.001, 'batch_size': 256, 'user_item_mlp_dims': [128, 64], 'final_mlp_dims': [64, 32], 'dropout_rate': 0.1}\n",
      "\n",
      "--- Trial 3/10 ---\n",
      "Current Parameters: {'embedding_dim': 128, 'text_encoder_hidden_dim': 256, 'learning_rate': 0.002, 'batch_size': 128, 'user_item_mlp_dims': [64, 32], 'final_mlp_dims': [32, 16], 'dropout_rate': 0.2}\n",
      "  Trial 3 Test RMSE: 0.9331\n",
      "\n",
      "--- Trial 4/10 ---\n",
      "Current Parameters: {'embedding_dim': 128, 'text_encoder_hidden_dim': 256, 'learning_rate': 0.002, 'batch_size': 512, 'user_item_mlp_dims': [128, 64], 'final_mlp_dims': [64, 32], 'dropout_rate': 0.2}\n",
      "  Trial 4 Test RMSE: 0.9336\n",
      "\n",
      "--- Trial 5/10 ---\n",
      "Current Parameters: {'embedding_dim': 64, 'text_encoder_hidden_dim': 256, 'learning_rate': 0.0005, 'batch_size': 256, 'user_item_mlp_dims': [64, 32], 'final_mlp_dims': [32, 16], 'dropout_rate': 0.3}\n",
      "  Trial 5 Test RMSE: 0.8873\n",
      "  --> New best RMSE found: 0.8873 with params: {'embedding_dim': 64, 'text_encoder_hidden_dim': 256, 'learning_rate': 0.0005, 'batch_size': 256, 'user_item_mlp_dims': [64, 32], 'final_mlp_dims': [32, 16], 'dropout_rate': 0.3}\n",
      "\n",
      "--- Trial 6/10 ---\n",
      "Current Parameters: {'embedding_dim': 64, 'text_encoder_hidden_dim': 128, 'learning_rate': 0.001, 'batch_size': 128, 'user_item_mlp_dims': [256, 128], 'final_mlp_dims': [64, 32], 'dropout_rate': 0.1}\n",
      "  Trial 6 Test RMSE: 0.8908\n",
      "\n",
      "--- Trial 7/10 ---\n",
      "Current Parameters: {'embedding_dim': 64, 'text_encoder_hidden_dim': 256, 'learning_rate': 0.002, 'batch_size': 512, 'user_item_mlp_dims': [64, 32], 'final_mlp_dims': [128, 64], 'dropout_rate': 0.2}\n",
      "  Trial 7 Test RMSE: 0.9140\n",
      "\n",
      "--- Trial 8/10 ---\n",
      "Current Parameters: {'embedding_dim': 64, 'text_encoder_hidden_dim': 64, 'learning_rate': 0.002, 'batch_size': 256, 'user_item_mlp_dims': [256, 128], 'final_mlp_dims': [32, 16], 'dropout_rate': 0.3}\n",
      "  Trial 8 Test RMSE: 0.9402\n",
      "\n",
      "--- Trial 9/10 ---\n",
      "Current Parameters: {'embedding_dim': 128, 'text_encoder_hidden_dim': 256, 'learning_rate': 0.001, 'batch_size': 256, 'user_item_mlp_dims': [256, 128], 'final_mlp_dims': [64, 32], 'dropout_rate': 0.2}\n",
      "  Trial 9 Test RMSE: 0.9221\n",
      "\n",
      "--- Trial 10/10 ---\n",
      "Current Parameters: {'embedding_dim': 64, 'text_encoder_hidden_dim': 256, 'learning_rate': 0.001, 'batch_size': 128, 'user_item_mlp_dims': [64, 32], 'final_mlp_dims': [32, 16], 'dropout_rate': 0.3}\n",
      "  Trial 10 Test RMSE: 0.9281\n",
      "\n",
      "--- Hyperparameter Search Completed ---\n",
      "Best RMSE found: 0.8873\n",
      "Best Parameters: {'embedding_dim': 64, 'text_encoder_hidden_dim': 256, 'learning_rate': 0.0005, 'batch_size': 256, 'user_item_mlp_dims': [64, 32], 'final_mlp_dims': [32, 16], 'dropout_rate': 0.3}\n",
      "Full results logged to 'hyperparameter_search_results.json'\n",
      "Best model weights saved to 'best_overall_afram_model.pt'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "import random\n",
    "import json # Import json for saving results\n",
    "\n",
    "# --- 유틸리티 함수 ---\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates MAPE, preventing division by zero.\n",
    "    \"\"\"\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    non_zero_true = y_true != 0\n",
    "    if np.sum(non_zero_true) == 0:\n",
    "        return 0.0\n",
    "    return np.mean(np.abs((y_true[non_zero_true] - y_pred[non_zero_true]) / y_true[non_zero_true])) * 100\n",
    "\n",
    "# --- 장치 설정 (GPU 사용 가능 시) ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- 데이터 전처리 및 어휘 구축 ---\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Converts text to lowercase, keeps only alphabets, numbers, and spaces, then tokenizes.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    return text.split()\n",
    "\n",
    "class Vocabulary:\n",
    "    \"\"\"\n",
    "    Builds a vocabulary from text data and converts words to integer IDs.\n",
    "    \"\"\"\n",
    "    def __init__(self, min_freq):\n",
    "        self.stoi = {\"<PAD>\": 0, \"<UNK>\": 1} # string_to_int: Defines padding and unknown tokens\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<UNK>\"} # int_to_string\n",
    "        self.freq = Counter()\n",
    "        self.min_freq = min_freq\n",
    "    \n",
    "    def build_vocabulary(self, text_list):\n",
    "        \"\"\"\n",
    "        Builds the vocabulary based on the given text list.\n",
    "        Words appearing less than min_freq times are treated as <UNK> tokens.\n",
    "        \"\"\"\n",
    "        for text in text_list:\n",
    "            self.freq.update(text)\n",
    "        \n",
    "        idx = 2 # Start index after <PAD> and <UNK>\n",
    "        for word, count in self.freq.items():\n",
    "            if count >= self.min_freq:\n",
    "                self.stoi[word] = idx\n",
    "                self.itos[idx] = word\n",
    "                idx += 1\n",
    "    \n",
    "    def numericalize(self, text):\n",
    "        \"\"\"\n",
    "        Converts text (list of words) to a sequence of integer IDs.\n",
    "        \"\"\"\n",
    "        return [self.stoi.get(token, self.stoi[\"<UNK>\"]) for token in text]\n",
    "\n",
    "# --- 파일 로드 ---\n",
    "try:\n",
    "    df = pd.read_json('review.json', lines=True) # Use lines=True for JSONL format\n",
    "except ValueError:\n",
    "    print(\"Trying to read JSON without lines=True (assuming a single JSON object or array of objects).\")\n",
    "    df = pd.read_json('review.json') # Fallback for standard JSON\n",
    "\n",
    "# --- 필요한 컬럼 추출 및 인코딩 ---\n",
    "df_processed = df[['user_id', 'business_id', 'stars', 'text']].copy()\n",
    "\n",
    "user_encoder = LabelEncoder()\n",
    "business_encoder = LabelEncoder()\n",
    "\n",
    "df_processed.loc[:, 'user_encoded'] = user_encoder.fit_transform(df_processed['user_id'])\n",
    "df_processed.loc[:, 'business_encoded'] = business_encoder.fit_transform(df_processed['business_id'])\n",
    "\n",
    "num_users = len(user_encoder.classes_)\n",
    "num_businesses = len(business_encoder.classes_)\n",
    "\n",
    "# --- 텍스트 전처리 및 어휘 구축 실행 ---\n",
    "all_texts = df_processed['text'].apply(preprocess_text).tolist()\n",
    "min_word_freq = 5 # Set minimum word frequency (adjustable)\n",
    "vocab = Vocabulary(min_word_freq)\n",
    "vocab.build_vocabulary(all_texts)\n",
    "vocab_size = len(vocab.stoi)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# Convert review text to integer ID sequences and apply padding/truncation\n",
    "MAX_REVIEW_LEN = 100 # Maximum length of review text (adjustable)\n",
    "df_processed.loc[:, 'numericalized_text'] = df_processed['text'].apply(vocab.numericalize)\n",
    "df_processed['numericalized_text'] = df_processed['numericalized_text'].apply(\n",
    "    lambda x: x[:MAX_REVIEW_LEN] if len(x) > MAX_REVIEW_LEN else x + [vocab.stoi[\"<PAD>\"]] * (MAX_REVIEW_LEN - len(x))\n",
    ")\n",
    "\n",
    "# --- PyTorch Dataset 및 DataLoader 정의 ---\n",
    "class AFRAMDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset class for AFRAM model training.\n",
    "    Returns user ID, business ID, numericalized review text, and star rating.\n",
    "    \"\"\"\n",
    "    def __init__(self, df):\n",
    "        self.user_ids = torch.tensor(df['user_encoded'].values, dtype=torch.long)\n",
    "        self.business_ids = torch.tensor(df['business_encoded'].values, dtype=torch.long)\n",
    "        self.reviews = torch.tensor(np.array(df['numericalized_text'].tolist()), dtype=torch.long)\n",
    "        self.stars = torch.tensor(df['stars'].values, dtype=torch.float)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.stars)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.user_ids[idx], self.business_ids[idx], self.reviews[idx], self.stars[idx]\n",
    "\n",
    "# --- AFRAM 모델 아키텍처 정의 ---\n",
    "class TextEncoderWithAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Extracts features from review text using CNN, LSTM, and attention mechanism.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, dropout_rate):\n",
    "        super(TextEncoderWithAttention, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        self.conv = nn.Conv1d(in_channels=embedding_dim, out_channels=hidden_dim, kernel_size=3, padding=1)\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.attn_proj = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_dim * 2, 1))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, text_seq):\n",
    "        embedded = self.embedding(text_seq)\n",
    "        embedded = embedded.permute(0, 2, 1)\n",
    "        \n",
    "        conv_out = torch.relu(self.conv(embedded))\n",
    "        conv_out = conv_out.permute(0, 2, 1)\n",
    "\n",
    "        lstm_out, _ = self.lstm(self.dropout(conv_out))\n",
    "        \n",
    "        attn_weights = torch.tanh(self.attn_proj(lstm_out))\n",
    "        v_expanded = self.v.unsqueeze(0).expand(attn_weights.shape[0], -1, -1)\n",
    "        \n",
    "        scores = torch.bmm(attn_weights, v_expanded)\n",
    "        attention_weights = torch.softmax(scores, dim=1)\n",
    "        \n",
    "        context_vector = torch.sum(lstm_out * attention_weights, dim=1)\n",
    "        \n",
    "        return context_vector\n",
    "\n",
    "class AFRAMModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the full model structure from the AFRAM paper.\n",
    "    Combines user-business interaction and review text features to predict ratings.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_users, num_businesses, vocab_size, embedding_dim,\n",
    "                 text_encoder_hidden_dim, user_item_mlp_dims, final_mlp_dims, dropout_rate):\n",
    "        super(AFRAMModel, self).__init__()\n",
    "        \n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.business_embedding = nn.Embedding(num_businesses, embedding_dim)\n",
    "        \n",
    "        self.review_encoder = TextEncoderWithAttention(vocab_size, embedding_dim, text_encoder_hidden_dim, dropout_rate)\n",
    "\n",
    "        user_item_mlp_input_dim = embedding_dim * 2\n",
    "        user_item_layers = []\n",
    "        for dim in user_item_mlp_dims:\n",
    "            user_item_layers.append(nn.Linear(user_item_mlp_input_dim, dim))\n",
    "            user_item_layers.append(nn.ReLU())\n",
    "            user_item_mlp_input_dim = dim\n",
    "        self.user_item_mlp = nn.Sequential(*user_item_layers)\n",
    "        self.user_item_mlp_output_dim = user_item_mlp_dims[-1] if user_item_mlp_dims else embedding_dim * 2\n",
    "\n",
    "        final_mlp_input_dim = self.user_item_mlp_output_dim + \\\n",
    "                              text_encoder_hidden_dim * 2\n",
    "        \n",
    "        final_layers = []\n",
    "        for dim in final_mlp_dims:\n",
    "            final_layers.append(nn.Linear(final_mlp_input_dim, dim))\n",
    "            final_layers.append(nn.ReLU())\n",
    "            final_mlp_input_dim = dim\n",
    "        final_layers.append(nn.Linear(final_mlp_input_dim, 1))\n",
    "        self.prediction_mlp = nn.Sequential(*final_layers)\n",
    "\n",
    "    def forward(self, user_ids, business_ids, reviews):\n",
    "        user_vec = self.user_embedding(user_ids)\n",
    "        business_vec = self.business_embedding(business_ids)\n",
    "        \n",
    "        user_item_combined = torch.cat((user_vec, business_vec), dim=1)\n",
    "        user_item_features = self.user_item_mlp(user_item_combined)\n",
    "\n",
    "        review_features = self.review_encoder(reviews)\n",
    "        \n",
    "        combined_features = torch.cat((user_item_features, review_features), dim=1)\n",
    "        \n",
    "        predicted_rating = self.prediction_mlp(combined_features)\n",
    "        return predicted_rating.squeeze()\n",
    "\n",
    "# --- 하이퍼파라미터 탐색 공간 정의 ---\n",
    "param_grid = {\n",
    "    'embedding_dim': [32, 64, 128],\n",
    "    'text_encoder_hidden_dim': [64, 128, 256],\n",
    "    'learning_rate': [0.0005, 0.001, 0.002],\n",
    "    'batch_size': [128, 256, 512],\n",
    "    'user_item_mlp_dims': [[64, 32], [128, 64], [256, 128]],\n",
    "    'final_mlp_dims': [[32, 16], [64, 32], [128, 64]],\n",
    "    'dropout_rate': [0.1, 0.2, 0.3]\n",
    "}\n",
    "\n",
    "num_trials = 10 # Number of random combinations to try\n",
    "best_params = None\n",
    "best_rmse = float('inf')\n",
    "results_log = []\n",
    "\n",
    "# Fixed random state for data splitting for consistency across hyperparameter trials\n",
    "# You typically want to use the same data split for all trials when tuning.\n",
    "DATA_SPLIT_RANDOM_STATE = 42\n",
    "\n",
    "print(f\"\\n--- Starting Hyperparameter Search with {num_trials} trials ---\")\n",
    "print(f\"Data Split Random State: {DATA_SPLIT_RANDOM_STATE}\")\n",
    "\n",
    "# Data splitting (7:1:2 ratio) - fixed for hyperparameter tuning\n",
    "train_val_df, test_df = train_test_split(df_processed, test_size=0.2, random_state=DATA_SPLIT_RANDOM_STATE)\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=1/8, random_state=DATA_SPLIT_RANDOM_STATE)\n",
    "\n",
    "print(f\"Fixed Data Split: Train={len(train_df)}, Val={len(val_df)}, Test={len(test_df)}\")\n",
    "\n",
    "# Create Dataset objects (once for all trials)\n",
    "train_dataset = AFRAMDataset(train_df)\n",
    "val_dataset = AFRAMDataset(val_df)\n",
    "test_dataset = AFRAMDataset(test_df)\n",
    "\n",
    "for trial_num in range(num_trials):\n",
    "    print(f\"\\n--- Trial {trial_num + 1}/{num_trials} ---\")\n",
    "\n",
    "    # Randomly sample a set of hyperparameters for this trial\n",
    "    current_params = {k: random.choice(v) for k, v in param_grid.items()}\n",
    "    print(f\"Current Parameters: {current_params}\")\n",
    "\n",
    "    # Create DataLoaders for the current trial (batch_size might change)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=current_params['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=current_params['batch_size'], shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=current_params['batch_size'], shuffle=False)\n",
    "\n",
    "    # Unpack parameters\n",
    "    embedding_dim = current_params['embedding_dim']\n",
    "    text_encoder_hidden_dim = current_params['text_encoder_hidden_dim']\n",
    "    learning_rate = current_params['learning_rate']\n",
    "    batch_size = current_params['batch_size']\n",
    "    user_item_mlp_dims = current_params['user_item_mlp_dims']\n",
    "    final_mlp_dims = current_params['final_mlp_dims']\n",
    "    dropout_rate = current_params['dropout_rate']\n",
    "\n",
    "    epochs = 50 # Maximum number of epochs for each trial\n",
    "    patience = 7 # Slightly reduced patience for faster trials\n",
    "    min_delta = 0.0005\n",
    "\n",
    "    # Create new model instance and move to GPU for each trial\n",
    "    model = AFRAMModel(num_users, num_businesses, vocab_size, embedding_dim,\n",
    "                       text_encoder_hidden_dim, user_item_mlp_dims, final_mlp_dims, dropout_rate).to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    trial_best_val_rmse = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    trial_model_save_path = f'temp_best_model_trial_{trial_num+1}.pt' # Temporary model path for each trial\n",
    "\n",
    "    # --- Training Loop (with Early Stopping) ---\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for user_ids, business_ids, reviews, stars in train_loader:\n",
    "            user_ids, business_ids, reviews, stars = user_ids.to(device), business_ids.to(device), reviews.to(device), stars.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(user_ids, business_ids, reviews)\n",
    "            loss = criterion(predictions, stars)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        val_predictions = []\n",
    "        val_true_ratings = []\n",
    "        with torch.no_grad():\n",
    "            for user_ids, business_ids, reviews, stars in val_loader:\n",
    "                user_ids, business_ids, reviews, stars = user_ids.to(device), business_ids.to(device), reviews.to(device), stars.to(device)\n",
    "                predictions = model(user_ids, business_ids, reviews)\n",
    "                loss = criterion(predictions, stars)\n",
    "                total_val_loss += loss.item()\n",
    "                val_predictions.extend(predictions.tolist())\n",
    "                val_true_ratings.extend(stars.tolist())\n",
    "\n",
    "        current_val_rmse = np.sqrt(mean_squared_error(val_true_ratings, val_predictions))\n",
    "\n",
    "        if current_val_rmse < trial_best_val_rmse - min_delta:\n",
    "            trial_best_val_rmse = current_val_rmse\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), trial_model_save_path)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == patience:\n",
    "                # print(f\"    Early stopping at epoch {epoch+1}.\") # Can uncomment for more verbose output\n",
    "                break\n",
    "    \n",
    "    # --- Evaluate the best model from this trial on the Test Set ---\n",
    "    if os.path.exists(trial_model_save_path):\n",
    "        model.load_state_dict(torch.load(trial_model_save_path))\n",
    "    else:\n",
    "        print(f\"Warning: Model for trial {trial_num+1} not saved. Testing with last state.\")\n",
    "\n",
    "    model.eval()\n",
    "    test_predictions = []\n",
    "    true_ratings = []\n",
    "    with torch.no_grad():\n",
    "        for user_ids, business_ids, reviews, stars in test_loader:\n",
    "            user_ids, business_ids, reviews, stars = user_ids.to(device), business_ids.to(device), reviews.to(device), stars.to(device)\n",
    "            predictions = model(user_ids, business_ids, reviews)\n",
    "            test_predictions.extend(predictions.tolist())\n",
    "            true_ratings.extend(stars.tolist())\n",
    "\n",
    "    mse = mean_squared_error(true_ratings, test_predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(true_ratings, test_predictions)\n",
    "    mape = mean_absolute_percentage_error(true_ratings, test_predictions)\n",
    "\n",
    "    print(f\"  Trial {trial_num + 1} Test RMSE: {rmse:.4f}\")\n",
    "\n",
    "    # Log results for this trial\n",
    "    trial_results = {\n",
    "        'trial_num': trial_num + 1,\n",
    "        'parameters': current_params,\n",
    "        'test_mse': mse,\n",
    "        'test_rmse': rmse,\n",
    "        'test_mae': mae,\n",
    "        'test_mape': mape\n",
    "    }\n",
    "    results_log.append(trial_results)\n",
    "\n",
    "    # Check if this trial yielded the best RMSE so far\n",
    "    if rmse < best_rmse:\n",
    "        best_rmse = rmse\n",
    "        best_params = current_params\n",
    "        # Optionally save the best model found during hyperparameter search\n",
    "        torch.save(model.state_dict(), 'best_overall_afram_model.pt')\n",
    "        print(f\"  --> New best RMSE found: {best_rmse:.4f} with params: {best_params}\")\n",
    "\n",
    "    # Clean up temporary model file\n",
    "    if os.path.exists(trial_model_save_path):\n",
    "        os.remove(trial_model_save_path)\n",
    "\n",
    "# --- Final Output ---\n",
    "print(f\"\\n--- Hyperparameter Search Completed ---\")\n",
    "print(f\"Best RMSE found: {best_rmse:.4f}\")\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "# Optionally save the full results log to a JSON file\n",
    "with open('hyperparameter_search_results.json', 'w') as f:\n",
    "    json.dump(results_log, f, indent=4)\n",
    "print(f\"Full results logged to 'hyperparameter_search_results.json'\")\n",
    "print(f\"Best model weights saved to 'best_overall_afram_model.pt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfba2cf1-b35d-45bd-a7f5-00539300ca33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Vocabulary size: 51962\n",
      "\n",
      "--- Starting 5 runs of training and evaluation ---\n",
      "Base Random State: 42\n",
      "Parameters: {'embedding_dim': 64, 'text_encoder_hidden_dim': 256, 'learning_rate': 0.0005, 'batch_size': 256, 'user_item_mlp_dims': [64, 32], 'final_mlp_dims': [32, 16], 'dropout_rate': 0.3}\n",
      "\n",
      "--- Run 1/5 (Random State: 42) ---\n",
      "Run 1 Data Split: Train=313456, Val=44780, Test=89560\n",
      "  Early stopping! No improvement in RMSE for 10 epochs.\n",
      "Loaded best model weights from best_afram_model_run_1.pt\n",
      "--- Run 1 Performance on Test Set ---\n",
      "Mean Squared Error (MSE): 0.8070\n",
      "Root Mean Squared Error (RMSE): 0.8984\n",
      "Mean Absolute Error (MAE): 0.6806\n",
      "Mean Absolute Percentage Error (MAPE): 25.99%\n",
      "\n",
      "--- Run 2/5 (Random State: 43) ---\n",
      "Run 2 Data Split: Train=313456, Val=44780, Test=89560\n",
      "  Early stopping! No improvement in RMSE for 10 epochs.\n",
      "Loaded best model weights from best_afram_model_run_2.pt\n",
      "--- Run 2 Performance on Test Set ---\n",
      "Mean Squared Error (MSE): 0.8661\n",
      "Root Mean Squared Error (RMSE): 0.9307\n",
      "Mean Absolute Error (MAE): 0.7152\n",
      "Mean Absolute Percentage Error (MAPE): 27.24%\n",
      "\n",
      "--- Run 3/5 (Random State: 44) ---\n",
      "Run 3 Data Split: Train=313456, Val=44780, Test=89560\n",
      "  Early stopping! No improvement in RMSE for 10 epochs.\n",
      "Loaded best model weights from best_afram_model_run_3.pt\n",
      "--- Run 3 Performance on Test Set ---\n",
      "Mean Squared Error (MSE): 0.8201\n",
      "Root Mean Squared Error (RMSE): 0.9056\n",
      "Mean Absolute Error (MAE): 0.6850\n",
      "Mean Absolute Percentage Error (MAPE): 26.13%\n",
      "\n",
      "--- Run 4/5 (Random State: 45) ---\n",
      "Run 4 Data Split: Train=313456, Val=44780, Test=89560\n",
      "  Early stopping! No improvement in RMSE for 10 epochs.\n",
      "Loaded best model weights from best_afram_model_run_4.pt\n",
      "--- Run 4 Performance on Test Set ---\n",
      "Mean Squared Error (MSE): 0.8288\n",
      "Root Mean Squared Error (RMSE): 0.9104\n",
      "Mean Absolute Error (MAE): 0.6914\n",
      "Mean Absolute Percentage Error (MAPE): 26.78%\n",
      "\n",
      "--- Run 5/5 (Random State: 46) ---\n",
      "Run 5 Data Split: Train=313456, Val=44780, Test=89560\n",
      "  Early stopping! No improvement in RMSE for 10 epochs.\n",
      "Loaded best model weights from best_afram_model_run_5.pt\n",
      "--- Run 5 Performance on Test Set ---\n",
      "Mean Squared Error (MSE): 0.8507\n",
      "Root Mean Squared Error (RMSE): 0.9223\n",
      "Mean Absolute Error (MAE): 0.7034\n",
      "Mean Absolute Percentage Error (MAPE): 26.64%\n",
      "\n",
      "--- Average Performance over 5 Runs ---\n",
      "Average MSE: 0.8346 +/- 0.0212\n",
      "Average RMSE: 0.9135 +/- 0.0116\n",
      "Average MAE: 0.6951 +/- 0.0126\n",
      "Average MAPE: 26.56% +/- 0.45%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "# --- 유틸리티 함수 ---\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates MAPE, preventing division by zero.\n",
    "    \"\"\"\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    non_zero_true = y_true != 0\n",
    "    if np.sum(non_zero_true) == 0:\n",
    "        return 0.0\n",
    "    return np.mean(np.abs((y_true[non_zero_true] - y_pred[non_zero_true]) / y_true[non_zero_true])) * 100\n",
    "\n",
    "# --- 장치 설정 (GPU 사용 가능 시) ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- 데이터 전처리 및 어휘 구축 ---\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Converts text to lowercase, keeps only alphabets, numbers, and spaces, then tokenizes.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    return text.split()\n",
    "\n",
    "class Vocabulary:\n",
    "    \"\"\"\n",
    "    Builds a vocabulary from text data and converts words to integer IDs.\n",
    "    \"\"\"\n",
    "    def __init__(self, min_freq):\n",
    "        self.stoi = {\"<PAD>\": 0, \"<UNK>\": 1} # string_to_int: Defines padding and unknown tokens\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<UNK>\"} # int_to_string\n",
    "        self.freq = Counter()\n",
    "        self.min_freq = min_freq\n",
    "    \n",
    "    def build_vocabulary(self, text_list):\n",
    "        \"\"\"\n",
    "        Builds the vocabulary based on the given text list.\n",
    "        Words appearing less than min_freq times are treated as <UNK> tokens.\n",
    "        \"\"\"\n",
    "        for text in text_list:\n",
    "            self.freq.update(text)\n",
    "        \n",
    "        idx = 2 # Start index after <PAD> and <UNK>\n",
    "        for word, count in self.freq.items():\n",
    "            if count >= self.min_freq:\n",
    "                self.stoi[word] = idx\n",
    "                self.itos[idx] = word\n",
    "                idx += 1\n",
    "    \n",
    "    def numericalize(self, text):\n",
    "        \"\"\"\n",
    "        Converts text (list of words) to a sequence of integer IDs.\n",
    "        \"\"\"\n",
    "        return [self.stoi.get(token, self.stoi[\"<UNK>\"]) for token in text]\n",
    "\n",
    "# --- 파일 로드 ---\n",
    "# Changed to read review.json\n",
    "try:\n",
    "    df = pd.read_json('review.json', lines=True) # Use lines=True for JSONL format\n",
    "except ValueError:\n",
    "    print(\"Trying to read JSON without lines=True (assuming a single JSON object or array of objects).\")\n",
    "    df = pd.read_json('review.json') # Fallback for standard JSON\n",
    "\n",
    "# --- 필요한 컬럼 추출 및 인코딩 ---\n",
    "df_processed = df[['user_id', 'business_id', 'stars', 'text']].copy()\n",
    "\n",
    "user_encoder = LabelEncoder()\n",
    "business_encoder = LabelEncoder()\n",
    "\n",
    "df_processed.loc[:, 'user_encoded'] = user_encoder.fit_transform(df_processed['user_id'])\n",
    "df_processed.loc[:, 'business_encoded'] = business_encoder.fit_transform(df_processed['business_id'])\n",
    "\n",
    "num_users = len(user_encoder.classes_)\n",
    "num_businesses = len(business_encoder.classes_)\n",
    "\n",
    "# --- 텍스트 전처리 및 어휘 구축 실행 ---\n",
    "all_texts = df_processed['text'].apply(preprocess_text).tolist()\n",
    "min_word_freq = 5 # Set minimum word frequency (adjustable)\n",
    "vocab = Vocabulary(min_word_freq)\n",
    "vocab.build_vocabulary(all_texts)\n",
    "vocab_size = len(vocab.stoi)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# Convert review text to integer ID sequences and apply padding/truncation\n",
    "MAX_REVIEW_LEN = 100 # Maximum length of review text (adjustable)\n",
    "df_processed.loc[:, 'numericalized_text'] = df_processed['text'].apply(vocab.numericalize)\n",
    "df_processed['numericalized_text'] = df_processed['numericalized_text'].apply(\n",
    "    lambda x: x[:MAX_REVIEW_LEN] if len(x) > MAX_REVIEW_LEN else x + [vocab.stoi[\"<PAD>\"]] * (MAX_REVIEW_LEN - len(x))\n",
    ")\n",
    "\n",
    "# --- PyTorch Dataset 및 DataLoader 정의 ---\n",
    "class AFRAMDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset class for AFRAM model training.\n",
    "    Returns user ID, business ID, numericalized review text, and star rating.\n",
    "    \"\"\"\n",
    "    def __init__(self, df):\n",
    "        self.user_ids = torch.tensor(df['user_encoded'].values, dtype=torch.long)\n",
    "        self.business_ids = torch.tensor(df['business_encoded'].values, dtype=torch.long)\n",
    "        self.reviews = torch.tensor(np.array(df['numericalized_text'].tolist()), dtype=torch.long)\n",
    "        self.stars = torch.tensor(df['stars'].values, dtype=torch.float)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.stars)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.user_ids[idx], self.business_ids[idx], self.reviews[idx], self.stars[idx]\n",
    "\n",
    "# --- AFRAM 모델 아키텍처 정의 ---\n",
    "class TextEncoderWithAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Extracts features from review text using CNN, LSTM, and attention mechanism.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, dropout_rate):\n",
    "        super(TextEncoderWithAttention, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # Convolutional Layer (CNN)\n",
    "        self.conv = nn.Conv1d(in_channels=embedding_dim, out_channels=hidden_dim, kernel_size=3, padding=1)\n",
    "        # Bidirectional LSTM Layer\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # Attention Layer (Bahdanau-style Additive Attention)\n",
    "        self.attn_proj = nn.Linear(hidden_dim * 2, hidden_dim * 2) # Matches Bi-LSTM output dimension\n",
    "        self.v = nn.Parameter(torch.rand(hidden_dim * 2, 1)) # Learnable attention weight vector\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, text_seq):\n",
    "        # text_seq: (batch_size, seq_len)\n",
    "        embedded = self.embedding(text_seq) # (batch_size, seq_len, embedding_dim)\n",
    "        embedded = embedded.permute(0, 2, 1) # Change dimension for Conv1d (batch_size, embedding_dim, seq_len)\n",
    "        \n",
    "        conv_out = torch.relu(self.conv(embedded)) # (batch_size, hidden_dim, seq_len)\n",
    "        conv_out = conv_out.permute(0, 2, 1) # Change dimension for LSTM (batch_size, seq_len, hidden_dim)\n",
    "\n",
    "        lstm_out, _ = self.lstm(self.dropout(conv_out)) # (batch_size, seq_len, hidden_dim * 2) (Bi-LSTM)\n",
    "        \n",
    "        # Apply attention mechanism\n",
    "        attn_weights = torch.tanh(self.attn_proj(lstm_out)) # (batch_size, seq_len, hidden_dim * 2)\n",
    "        v_expanded = self.v.unsqueeze(0).expand(attn_weights.shape[0], -1, -1) # Expand v to match batch size\n",
    "        \n",
    "        scores = torch.bmm(attn_weights, v_expanded) # Calculate attention scores (batch_size, seq_len, 1)\n",
    "        attention_weights = torch.softmax(scores, dim=1) # Normalize weights with softmax (sum=1)\n",
    "        \n",
    "        # Calculate context vector (weighted sum): weighted average of LSTM output applying attention weights\n",
    "        context_vector = torch.sum(lstm_out * attention_weights, dim=1) # (batch_size, hidden_dim * 2)\n",
    "        \n",
    "        return context_vector # This vector represents the \"aspect features\" of the review.\n",
    "\n",
    "class AFRAMModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the full model structure from the AFRAM paper.\n",
    "    Combines user-business interaction and review text features to predict ratings.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_users, num_businesses, vocab_size, embedding_dim,\n",
    "                 text_encoder_hidden_dim, user_item_mlp_dims, final_mlp_dims, dropout_rate):\n",
    "        super(AFRAMModel, self).__init__()\n",
    "        \n",
    "        # User and business embedding layers\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.business_embedding = nn.Embedding(num_businesses, embedding_dim)\n",
    "        \n",
    "        # Module to encode review text (including attention)\n",
    "        self.review_encoder = TextEncoderWithAttention(vocab_size, embedding_dim, text_encoder_hidden_dim, dropout_rate)\n",
    "\n",
    "        # User-business interaction MLP (Customer-Restaurant Interaction Module in the paper)\n",
    "        user_item_mlp_input_dim = embedding_dim * 2\n",
    "        user_item_layers = []\n",
    "        for dim in user_item_mlp_dims:\n",
    "            user_item_layers.append(nn.Linear(user_item_mlp_input_dim, dim))\n",
    "            user_item_layers.append(nn.ReLU())\n",
    "            user_item_mlp_input_dim = dim\n",
    "        self.user_item_mlp = nn.Sequential(*user_item_layers)\n",
    "        self.user_item_mlp_output_dim = user_item_mlp_dims[-1] if user_item_mlp_dims else embedding_dim * 2\n",
    "\n",
    "        # Final rating prediction MLP (Rating Prediction Module in the paper)\n",
    "        final_mlp_input_dim = self.user_item_mlp_output_dim + \\\n",
    "                              text_encoder_hidden_dim * 2 # Output dimension of review_encoder (Bi-LSTM, so hidden_dim * 2)\n",
    "        \n",
    "        final_layers = []\n",
    "        for dim in final_mlp_dims:\n",
    "            final_layers.append(nn.Linear(final_mlp_input_dim, dim))\n",
    "            final_layers.append(nn.ReLU())\n",
    "            final_mlp_input_dim = dim\n",
    "        final_layers.append(nn.Linear(final_mlp_input_dim, 1)) # Final output is rating (1 dimension)\n",
    "        self.prediction_mlp = nn.Sequential(*final_layers)\n",
    "\n",
    "    def forward(self, user_ids, business_ids, reviews):\n",
    "        # Get user and business embedding vectors\n",
    "        user_vec = self.user_embedding(user_ids)\n",
    "        business_vec = self.business_embedding(business_ids)\n",
    "        \n",
    "        # Combine user-business embeddings and pass through MLP to generate interaction features\n",
    "        user_item_combined = torch.cat((user_vec, business_vec), dim=1)\n",
    "        user_item_features = self.user_item_mlp(user_item_combined)\n",
    "\n",
    "        # Pass review text through review encoder to extract text features (aspect features)\n",
    "        review_features = self.review_encoder(reviews)\n",
    "        \n",
    "        # Combine interaction features and review text features\n",
    "        combined_features = torch.cat((user_item_features, review_features), dim=1)\n",
    "        \n",
    "        # Pass through final rating prediction MLP to return result\n",
    "        predicted_rating = self.prediction_mlp(combined_features)\n",
    "        return predicted_rating.squeeze() # Reduce dimension to return 1D rating\n",
    "\n",
    "# --- 모델 학습 및 평가 (다중 반복 및 평균) ---\n",
    "# Hyperparameters to use (can be tuned)\n",
    "params = {\n",
    "    'embedding_dim': 64,\n",
    "    'text_encoder_hidden_dim': 256,\n",
    "    'learning_rate': 0.0005,\n",
    "    'batch_size': 256,\n",
    "    'user_item_mlp_dims': [64, 32],\n",
    "    'final_mlp_dims': [32, 16],\n",
    "    'dropout_rate': 0.3\n",
    "}\n",
    "\n",
    "# Lists to store results from each run\n",
    "all_mse = []\n",
    "all_rmse = []\n",
    "all_mae = []\n",
    "all_mape = []\n",
    "\n",
    "num_runs = 5 # Number of repetitions\n",
    "\n",
    "print(f\"\\n--- Starting {num_runs} runs of training and evaluation ---\")\n",
    "print(f\"Base Random State: 42\")\n",
    "print(f\"Parameters: {params}\")\n",
    "\n",
    "for i in range(num_runs):\n",
    "    current_random_state = 42 + i\n",
    "    print(f\"\\n--- Run {i+1}/{num_runs} (Random State: {current_random_state}) ---\")\n",
    "\n",
    "    # Data splitting (7:1:2 ratio) - split with a different random_state each iteration\n",
    "    train_val_df, test_df = train_test_split(df_processed, test_size=0.2, random_state=current_random_state)\n",
    "    train_df, val_df = train_test_split(train_val_df, test_size=1/8, random_state=current_random_state)\n",
    "\n",
    "    print(f\"Run {i+1} Data Split: Train={len(train_df)}, Val={len(val_df)}, Test={len(test_df)}\")\n",
    "\n",
    "    # Create Dataset objects\n",
    "    train_dataset = AFRAMDataset(train_df)\n",
    "    val_dataset = AFRAMDataset(val_df)\n",
    "    test_dataset = AFRAMDataset(test_df)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=params['batch_size'], shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=params['batch_size'], shuffle=False)\n",
    "\n",
    "    # Unpack parameters\n",
    "    embedding_dim = params['embedding_dim']\n",
    "    text_encoder_hidden_dim = params['text_encoder_hidden_dim']\n",
    "    learning_rate = params['learning_rate']\n",
    "    batch_size = params['batch_size']\n",
    "    user_item_mlp_dims = params['user_item_mlp_dims']\n",
    "    final_mlp_dims = params['final_mlp_dims']\n",
    "    dropout_rate = params['dropout_rate']\n",
    "\n",
    "    epochs = 50 # Maximum number of epochs\n",
    "    patience = 10 # Number of epochs to wait for validation performance improvement for early stopping\n",
    "    min_delta = 0.0005 # Minimum change to be considered an improvement\n",
    "\n",
    "    best_val_rmse = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    model_save_path = f'best_afram_model_run_{i+1}.pt' # Unique model save path for each run\n",
    "\n",
    "    # Create new model instance and move to GPU (new instance for each run)\n",
    "    model = AFRAMModel(num_users, num_businesses, vocab_size, embedding_dim,\n",
    "                       text_encoder_hidden_dim, user_item_mlp_dims, final_mlp_dims, dropout_rate).to(device)\n",
    "\n",
    "    criterion = nn.MSELoss() # Loss function: MSE\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) # Optimizer: Adam\n",
    "\n",
    "    # --- Training Loop (with Early Stopping) ---\n",
    "    for epoch in range(epochs):\n",
    "        model.train() # Set model to training mode\n",
    "        total_train_loss = 0\n",
    "        for user_ids, business_ids, reviews, stars in train_loader:\n",
    "            # Move data to GPU\n",
    "            user_ids, business_ids, reviews, stars = user_ids.to(device), business_ids.to(device), reviews.to(device), stars.to(device)\n",
    "            \n",
    "            optimizer.zero_grad() # Initialize optimizer gradients\n",
    "            predictions = model(user_ids, business_ids, reviews) # Perform prediction\n",
    "            loss = criterion(predictions, stars) # Calculate loss\n",
    "            loss.backward() # Backpropagation\n",
    "            optimizer.step() # Update parameters\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        model.eval() # Set model to evaluation mode\n",
    "        total_val_loss = 0\n",
    "        val_predictions = []\n",
    "        val_true_ratings = []\n",
    "        with torch.no_grad(): # Disable gradient calculation (save memory, speed up)\n",
    "            for user_ids, business_ids, reviews, stars in val_loader:\n",
    "                # Move data to GPU\n",
    "                user_ids, business_ids, reviews, stars = user_ids.to(device), business_ids.to(device), reviews.to(device), stars.to(device)\n",
    "                \n",
    "                predictions = model(user_ids, business_ids, reviews)\n",
    "                loss = criterion(predictions, stars)\n",
    "                total_val_loss += loss.item()\n",
    "                val_predictions.extend(predictions.tolist())\n",
    "                val_true_ratings.extend(stars.tolist())\n",
    "\n",
    "        current_val_rmse = np.sqrt(mean_squared_error(val_true_ratings, val_predictions))\n",
    "\n",
    "        # Early stopping logic\n",
    "        if current_val_rmse < best_val_rmse - min_delta:\n",
    "            best_val_rmse = current_val_rmse\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == patience:\n",
    "                print(f\"  Early stopping! No improvement in RMSE for {patience} epochs.\")\n",
    "                break\n",
    "\n",
    "    # --- Final Model Testing ---\n",
    "    if os.path.exists(model_save_path):\n",
    "        model.load_state_dict(torch.load(model_save_path))\n",
    "        print(f\"Loaded best model weights from {model_save_path}\")\n",
    "    else:\n",
    "        print(f\"Could not find best model weights at '{model_save_path}'. Testing with current model state.\")\n",
    "\n",
    "    model.eval()\n",
    "    test_predictions = []\n",
    "    true_ratings = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for user_ids, business_ids, reviews, stars in test_loader:\n",
    "            user_ids, business_ids, reviews, stars = user_ids.to(device), business_ids.to(device), reviews.to(device), stars.to(device)\n",
    "            predictions = model(user_ids, business_ids, reviews)\n",
    "            test_predictions.extend(predictions.tolist())\n",
    "            true_ratings.extend(stars.tolist())\n",
    "\n",
    "    mse = mean_squared_error(true_ratings, test_predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(true_ratings, test_predictions)\n",
    "    mape = mean_absolute_percentage_error(true_ratings, test_predictions)\n",
    "\n",
    "    print(f\"--- Run {i+1} Performance on Test Set ---\")\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "    print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")\n",
    "\n",
    "    all_mse.append(mse)\n",
    "    all_rmse.append(rmse)\n",
    "    all_mae.append(mae)\n",
    "    all_mape.append(mape)\n",
    "\n",
    "# --- Average Performance Output ---\n",
    "print(f\"\\n--- Average Performance over {num_runs} Runs ---\")\n",
    "print(f\"Average MSE: {np.mean(all_mse):.4f} +/- {np.std(all_mse):.4f}\")\n",
    "print(f\"Average RMSE: {np.mean(all_rmse):.4f} +/- {np.std(all_rmse):.4f}\")\n",
    "print(f\"Average MAE: {np.mean(all_mae):.4f} +/- {np.std(all_mae):.4f}\")\n",
    "print(f\"Average MAPE: {np.mean(all_mape):.2f}% +/- {np.std(all_mape):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb4b141-c818-45f8-937e-8123656a416d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
