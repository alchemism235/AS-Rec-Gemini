{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e6b07c6-fad9-43ef-8c68-d8e21223edce",
   "metadata": {},
   "source": [
    "jsonl파일을 float32로 변환하고 parquet 파일로 변경 - fl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f6ace07-395e-4c8b-8da1-bab595cf5a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parquet 파일 생성\n",
      " review_data_optimized_fl.parquet 로 저장\n",
      "저장 완료\n",
      "\n",
      " 데이터 프레임 정보\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500892 entries, 0 to 500891\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   review_id     500892 non-null  object\n",
      " 1   user_id       500892 non-null  object\n",
      " 2   business_id   500892 non-null  object\n",
      " 3   review_stars  500892 non-null  int64 \n",
      " 4   embedding     500892 non-null  object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 19.1+ MB\n",
      "None\n",
      "\n",
      " 첫 5개 행 데이터\n",
      "                review_id                 user_id             business_id  \\\n",
      "0  OAhBYw8IQ6wlfw1owXWRWw  1C2lxzUo1Hyye4RFIXly3g  BVndHaLihEYbr76Z0CMEGw   \n",
      "1  mO398Ed5dpv1H5ZsKc8KXw  yobeeTUBfaTBcnk26mXNuA  hKameFsaXh9g8WQbv593UA   \n",
      "2  PPgbLBvi34A6m7bKJfTwhw  3TL6HZ1JrKcNTvGDWKlrow  GyC36Pn0Q1-qHnqXys6yFg   \n",
      "3  LnKr0hwejzl71QmoQyTRDQ  7RU_xK1tEGlUvXfe0GvtEg  hAmuto6UndVroyd_DaD-TA   \n",
      "4  RMho6HMpdec1YgIypwWQrQ  SNngOVGTkD34B3FAFnMv5A  ab3pRv-b0o-BwMK2jVbH3Q   \n",
      "\n",
      "   review_stars                                          embedding  \n",
      "0             5  [-0.0060858615, -0.012740523, 0.03105425, -0.0...  \n",
      "1             4  [-0.007592755, -0.027990926, 0.018363668, -0.0...  \n",
      "2             1  [-0.0030736073, -0.016940113, 0.013264897, -0....  \n",
      "3             5  [-0.047213227, 0.0029964936, 0.008528922, -0.0...  \n",
      "4             5  [-0.028922588, -0.008392463, 0.00927257, -0.05...  \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "def process_and_save_data(jsonl_file_path, parquet_file_path):\n",
    "\n",
    "    print(f\"parquet 파일 생성\")\n",
    "\n",
    "    other_data_list = []\n",
    "    embeddings_list = []\n",
    "\n",
    "    try:\n",
    "        with open(jsonl_file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                embedding_vector = data.pop('embedding')\n",
    "                embeddings_list.append(np.array(embedding_vector, dtype=np.float32))\n",
    "                other_data_list.append(data)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"파일을 찾을 수 없음\")\n",
    "        return\n",
    "    except (json.JSONDecodeError, KeyError) as e:\n",
    "        print(f\"JSON 처리 중 문제 발생 - {e} at line: {line[:50]}...\")\n",
    "        return\n",
    "\n",
    "    # 데이터프레임 생성\n",
    "    df = pd.DataFrame(other_data_list)\n",
    "    \n",
    "    # 임베딩 리스트를 한 열로 추가\n",
    "    df['embedding'] = embeddings_list\n",
    "\n",
    "    # 데이터 프레임을 parquet 파일로 저장\n",
    "    print(f\" {parquet_file_path} 로 저장\")\n",
    "    df.to_parquet(parquet_file_path, engine='pyarrow', compression='snappy')\n",
    "    print(f\"저장 완료\")\n",
    "\n",
    "jsonl_file = 'dataset_fl_with_embeddings_vector.jsonl'\n",
    "parquet_file = 'review_data_optimized_fl.parquet'\n",
    "\n",
    "# 데이터 처리 및 저장 함수 실행\n",
    "process_and_save_data(jsonl_file, parquet_file)\n",
    "\n",
    "# 데이터 로드 및 확인\n",
    "\n",
    "try:\n",
    "    df_loaded = pd.read_parquet(parquet_file)\n",
    "    \n",
    "    # 로드된 데이터 정보 확인\n",
    "    print(\"\\n 데이터 프레임 정보\")\n",
    "    print(df_loaded.info())\n",
    "\n",
    "    print(\"\\n 첫 5개 행 데이터\")\n",
    "    print(df_loaded.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"오류 - 파일을 찾을 수 없음\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0359022e-8174-4ded-948e-1af3df738629",
   "metadata": {},
   "source": [
    "jsonl파일을 float32로 변환하고 parquet 파일로 변경 - la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52061cef-0168-47e4-9c18-447621101cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parquet 파일 생성\n",
      " review_data_optimized_la.parquet 로 저장\n",
      "저장 완료\n",
      "\n",
      " 데이터 프레임 정보\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 277766 entries, 0 to 277765\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   review_id     277766 non-null  object\n",
      " 1   user_id       277766 non-null  object\n",
      " 2   business_id   277766 non-null  object\n",
      " 3   review_stars  277766 non-null  int64 \n",
      " 4   embedding     277766 non-null  object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 10.6+ MB\n",
      "None\n",
      "\n",
      " 첫 5개 행 데이터\n",
      "                review_id                 user_id             business_id  \\\n",
      "0  Sx8TMOWLNuJBWer-0pcmoA  bcjbaE6dDog4jkNY91ncLQ  e4Vwtrqf-wpJfwesgvdgxQ   \n",
      "1  jC-fGfx-YLqxVBcyTAd4Pw  EBa-0-6AKoy6jziNexDJtg  W4ZEKkva9HpAdZG88juwyQ   \n",
      "2  A4n4YaE-owOVgTQcrVqHUw  S7bjj-L07JuRr-tpX1UZLw  I6L0Zxi5Ww0zEWSAVgngeQ   \n",
      "3  fGYcFOHfQL4stYPdD3J47g  CgyCtH9CbLO7J_uO3cL7OA  PzhPMkaNYiKDTHoTG0r8rw   \n",
      "4  zRIH5eDA2GbynjZPrk8dDg  07aVWNhBhpwqW3MdqoKDKQ  jLerIZFpe25jOUf722qzBA   \n",
      "\n",
      "   review_stars                                          embedding  \n",
      "0             4  [-0.01736502, -0.020718252, 0.020934245, -0.05...  \n",
      "1             3  [0.018170908, -0.035148412, -0.0028036707, -0....  \n",
      "2             4  [-0.012972604, -0.015292769, 0.024341572, -0.0...  \n",
      "3             4  [-0.0066076317, -0.009752215, 0.0018852043, -0...  \n",
      "4             3  [-0.0004076383, -0.015056654, 0.022396443, -0....  \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "def process_and_save_data(jsonl_file_path, parquet_file_path):\n",
    "\n",
    "    print(f\"parquet 파일 생성\")\n",
    "\n",
    "    other_data_list = []\n",
    "    embeddings_list = []\n",
    "\n",
    "    try:\n",
    "        with open(jsonl_file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                embedding_vector = data.pop('embedding')\n",
    "                embeddings_list.append(np.array(embedding_vector, dtype=np.float32))\n",
    "                other_data_list.append(data)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"파일을 찾을 수 없음\")\n",
    "        return\n",
    "    except (json.JSONDecodeError, KeyError) as e:\n",
    "        print(f\"JSON 처리 중 문제 발생 - {e} at line: {line[:50]}...\")\n",
    "        return\n",
    "\n",
    "    # 데이터프레임 생성\n",
    "    df = pd.DataFrame(other_data_list)\n",
    "    \n",
    "    # 임베딩 리스트를 한 열로 추가\n",
    "    df['embedding'] = embeddings_list\n",
    "\n",
    "    # 데이터 프레임을 parquet 파일로 저장\n",
    "    print(f\" {parquet_file_path} 로 저장\")\n",
    "    df.to_parquet(parquet_file_path, engine='pyarrow', compression='snappy')\n",
    "    print(f\"저장 완료\")\n",
    "\n",
    "jsonl_file = 'dataset_la_with_embeddings_vector.jsonl'\n",
    "parquet_file = 'review_data_optimized_la.parquet'\n",
    "\n",
    "# 데이터 처리 및 저장 함수 실행\n",
    "process_and_save_data(jsonl_file, parquet_file)\n",
    "\n",
    "# 데이터 로드 및 확인\n",
    "\n",
    "try:\n",
    "    df_loaded = pd.read_parquet(parquet_file)\n",
    "    \n",
    "    # 로드된 데이터 정보 확인\n",
    "    print(\"\\n 데이터 프레임 정보\")\n",
    "    print(df_loaded.info())\n",
    "\n",
    "    print(\"\\n 첫 5개 행 데이터\")\n",
    "    print(df_loaded.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"오류 - 파일을 찾을 수 없음\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1291d5c1-8b9b-4248-95cc-1220a5b34f0e",
   "metadata": {},
   "source": [
    "jsonl파일을 float32로 변환하고 parquet 파일로 변경 - pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc79350e-27b0-49e3-a963-dfeef69fe8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parquet 파일 생성\n",
      " review_data_optimized_pa.parquet 로 저장\n",
      "저장 완료\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "List index overflow.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# 데이터 로드 및 확인\u001b[39;00m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     df_loaded = pd.read_parquet(parquet_file)\n\u001b[32m     50\u001b[39m     \u001b[38;5;66;03m# 로드된 데이터 정보 확인\u001b[39;00m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m 데이터 프레임 정보\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parquet.py:669\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    666\u001b[39m     use_nullable_dtypes = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    667\u001b[39m check_dtype_backend(dtype_backend)\n\u001b[32m--> \u001b[39m\u001b[32m669\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m impl.read(\n\u001b[32m    670\u001b[39m     path,\n\u001b[32m    671\u001b[39m     columns=columns,\n\u001b[32m    672\u001b[39m     filters=filters,\n\u001b[32m    673\u001b[39m     storage_options=storage_options,\n\u001b[32m    674\u001b[39m     use_nullable_dtypes=use_nullable_dtypes,\n\u001b[32m    675\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m    676\u001b[39m     filesystem=filesystem,\n\u001b[32m    677\u001b[39m     **kwargs,\n\u001b[32m    678\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parquet.py:265\u001b[39m, in \u001b[36mPyArrowImpl.read\u001b[39m\u001b[34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[39m\n\u001b[32m    258\u001b[39m path_or_handle, handles, filesystem = _get_path_or_handle(\n\u001b[32m    259\u001b[39m     path,\n\u001b[32m    260\u001b[39m     filesystem,\n\u001b[32m    261\u001b[39m     storage_options=storage_options,\n\u001b[32m    262\u001b[39m     mode=\u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    263\u001b[39m )\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m     pa_table = \u001b[38;5;28mself\u001b[39m.api.parquet.read_table(\n\u001b[32m    266\u001b[39m         path_or_handle,\n\u001b[32m    267\u001b[39m         columns=columns,\n\u001b[32m    268\u001b[39m         filesystem=filesystem,\n\u001b[32m    269\u001b[39m         filters=filters,\n\u001b[32m    270\u001b[39m         **kwargs,\n\u001b[32m    271\u001b[39m     )\n\u001b[32m    273\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m catch_warnings():\n\u001b[32m    274\u001b[39m         filterwarnings(\n\u001b[32m    275\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    276\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmake_block is deprecated\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    277\u001b[39m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m    278\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\parquet\\core.py:1898\u001b[39m, in \u001b[36mread_table\u001b[39m\u001b[34m(source, columns, use_threads, schema, use_pandas_metadata, read_dictionary, binary_type, list_type, memory_map, buffer_size, partitioning, filesystem, filters, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification, arrow_extensions_enabled)\u001b[39m\n\u001b[32m   1884\u001b[39m     \u001b[38;5;66;03m# TODO test that source is not a directory or a list\u001b[39;00m\n\u001b[32m   1885\u001b[39m     dataset = ParquetFile(\n\u001b[32m   1886\u001b[39m         source, read_dictionary=read_dictionary,\n\u001b[32m   1887\u001b[39m         binary_type=binary_type,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1895\u001b[39m         page_checksum_verification=page_checksum_verification,\n\u001b[32m   1896\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1898\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m dataset.read(columns=columns, use_threads=use_threads,\n\u001b[32m   1899\u001b[39m                     use_pandas_metadata=use_pandas_metadata)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\parquet\\core.py:1538\u001b[39m, in \u001b[36mParquetDataset.read\u001b[39m\u001b[34m(self, columns, use_threads, use_pandas_metadata)\u001b[39m\n\u001b[32m   1530\u001b[39m         index_columns = [\n\u001b[32m   1531\u001b[39m             col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m _get_pandas_index_columns(metadata)\n\u001b[32m   1532\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, \u001b[38;5;28mdict\u001b[39m)\n\u001b[32m   1533\u001b[39m         ]\n\u001b[32m   1534\u001b[39m         columns = (\n\u001b[32m   1535\u001b[39m             \u001b[38;5;28mlist\u001b[39m(columns) + \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(index_columns) - \u001b[38;5;28mset\u001b[39m(columns))\n\u001b[32m   1536\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1538\u001b[39m table = \u001b[38;5;28mself\u001b[39m._dataset.to_table(\n\u001b[32m   1539\u001b[39m     columns=columns, \u001b[38;5;28mfilter\u001b[39m=\u001b[38;5;28mself\u001b[39m._filter_expression,\n\u001b[32m   1540\u001b[39m     use_threads=use_threads\n\u001b[32m   1541\u001b[39m )\n\u001b[32m   1543\u001b[39m \u001b[38;5;66;03m# if use_pandas_metadata, restore the pandas metadata (which gets\u001b[39;00m\n\u001b[32m   1544\u001b[39m \u001b[38;5;66;03m# lost if doing a specific `columns` selection in to_table)\u001b[39;00m\n\u001b[32m   1545\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_pandas_metadata:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\_dataset.pyx:589\u001b[39m, in \u001b[36mpyarrow._dataset.Dataset.to_table\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\_dataset.pyx:3939\u001b[39m, in \u001b[36mpyarrow._dataset.Scanner.to_table\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\error.pxi:155\u001b[39m, in \u001b[36mpyarrow.lib.pyarrow_internal_check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\error.pxi:92\u001b[39m, in \u001b[36mpyarrow.lib.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mOSError\u001b[39m: List index overflow."
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "def process_and_save_data(jsonl_file_path, parquet_file_path):\n",
    "\n",
    "    print(f\"parquet 파일 생성\")\n",
    "\n",
    "    other_data_list = []\n",
    "    embeddings_list = []\n",
    "\n",
    "    try:\n",
    "        with open(jsonl_file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                embedding_vector = data.pop('embedding')\n",
    "                embeddings_list.append(np.array(embedding_vector, dtype=np.float32))\n",
    "                other_data_list.append(data)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"파일을 찾을 수 없음\")\n",
    "        return\n",
    "    except (json.JSONDecodeError, KeyError) as e:\n",
    "        print(f\"JSON 처리 중 문제 발생 - {e} at line: {line[:50]}...\")\n",
    "        return\n",
    "\n",
    "    # 데이터프레임 생성\n",
    "    df = pd.DataFrame(other_data_list)\n",
    "    \n",
    "    # 임베딩 리스트를 한 열로 추가\n",
    "    df['embedding'] = embeddings_list\n",
    "\n",
    "    # 데이터 프레임을 parquet 파일로 저장\n",
    "    print(f\" {parquet_file_path} 로 저장\")\n",
    "    df.to_parquet(parquet_file_path, engine='pyarrow', compression='snappy')\n",
    "    print(f\"저장 완료\")\n",
    "\n",
    "jsonl_file = 'dataset_pa_with_embeddings_vector.jsonl'\n",
    "parquet_file = 'review_data_optimized_pa.parquet'\n",
    "\n",
    "# 데이터 처리 및 저장 함수 실행\n",
    "process_and_save_data(jsonl_file, parquet_file)\n",
    "\n",
    "# 데이터 로드 및 확인\n",
    "\n",
    "try:\n",
    "    df_loaded = pd.read_parquet(parquet_file)\n",
    "    \n",
    "    # 로드된 데이터 정보 확인\n",
    "    print(\"\\n 데이터 프레임 정보\")\n",
    "    print(df_loaded.info())\n",
    "\n",
    "    print(\"\\n 첫 5개 행 데이터\")\n",
    "    print(df_loaded.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"오류 - 파일을 찾을 수 없음\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1797008-c20b-448b-969c-1791199b738e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
